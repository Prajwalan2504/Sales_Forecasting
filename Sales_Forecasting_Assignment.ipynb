{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DAY 1"
      ],
      "metadata": {
        "id": "Lk9ERHKSJ1dd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "TN6myPBYh0oz",
        "outputId": "340ce776-2b53-4ba8-eaba-6b4655d6117a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'jax' has no attribute 'tree_util' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ce92ec86a8e0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marima\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mARIMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m \u001b[0;31m# line: 125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m \u001b[0;31m# line: 125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpsSet\u001b[0m \u001b[0;31m# line: 170\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtoco_convert\u001b[0m \u001b[0;31m# line: 1083\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauthoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelAnalyzer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAnalyzer\u001b[0m \u001b[0;31m# line: 35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpResolverType\u001b[0m \u001b[0;31m# line: 303\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatible\u001b[0m \u001b[0;31m# line: 263\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_wrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/authoring/authoring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverter_error_data_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_tf_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstablehlo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization_options_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquant_opts_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_phase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_phase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0m_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m# jax and rely on the names imported above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_derivatives\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcustom_derivatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_batching\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcustom_batching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_transpose\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcustom_transpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/custom_batching.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from jax._src.custom_batching import (\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mcustom_vmap\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcustom_vmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0msequential_vmap\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msequential_vmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/custom_batching.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/lax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    386\u001b[0m )\n\u001b[1;32m    387\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mad_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstop_gradient_p\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstop_gradient_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpjit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwith_sharding_constraint\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwith_sharding_constraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/lax/linalg.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from jax._src.lax.linalg import (\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mcholesky\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mcholesky_p\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcholesky_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/linalg.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meigh\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlax_eigh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlax\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlax_internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvd\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlax_svd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/eigh.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mqdwh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlax_linalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/stack.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_pytree_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'jax' has no attribute 'tree_util' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Data Cleaning**"
      ],
      "metadata": {
        "id": "wgNcCpdUQrC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "stores = pd.read_csv('stores.csv')\n",
        "oil = pd.read_csv('oil.csv')\n",
        "holidays = pd.read_csv('holidays_events.csv')"
      ],
      "metadata": {
        "id": "iycQpwJTikIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "nqJwGGsLjZko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "lyAQofmanKGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores.head()"
      ],
      "metadata": {
        "id": "BmT5IWK6nLIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oil.head()"
      ],
      "metadata": {
        "id": "kTR7EyvPnS5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holidays.head()"
      ],
      "metadata": {
        "id": "qYPxNnBinVo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train.columns:\n",
        "    print({i:train[i].unique()})  # To check unique values in train set"
      ],
      "metadata": {
        "id": "MMt6TCKQo8lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in test.columns:\n",
        "    print({i:test[i].unique()})  # To check unique values in test set"
      ],
      "metadata": {
        "id": "6cY4aCVcpPi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in stores.columns:\n",
        "    print({i:stores[i].unique()})  # To check unique values in stores set"
      ],
      "metadata": {
        "id": "tz6BhjNKpc1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in oil.columns:\n",
        "    print({i:oil[i].unique()})  # To check unique values in oil set"
      ],
      "metadata": {
        "id": "rfNz4cUspyTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in holidays.columns:\n",
        "    print({i:holidays[i].unique()})  # To check unique values in holidays set"
      ],
      "metadata": {
        "id": "RAZLxY4gp8uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values in Oil Prices\n",
        "oil['dcoilwtico'] = oil['dcoilwtico'].bfill().interpolate()  # Fill gaps with interpolation\n",
        "\n",
        "# Since in the first row is NaN so we are using Backword fill. This first fills\n",
        "# missing values using backward fill (copies the next value), then interpolates\n",
        "# any remaining gaps."
      ],
      "metadata": {
        "id": "Nl8psuDcnamj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oil.head()   # We can see that first row NaN is filled with next value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Im89jWMo58cY",
        "outputId": "7616bd55-309b-4cad-b9d0-c14c6ec45d19"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'oil' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-391b29931b90>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# We can see that first row NaN is filled with next value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'oil' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Converting Date Columns to Datetime\n",
        "train['date'] = pd.to_datetime(train['date'])\n",
        "test['date'] = pd.to_datetime(test['date'])\n",
        "oil['date'] = pd.to_datetime(oil['date'])\n",
        "holidays['date'] = pd.to_datetime(holidays['date'])"
      ],
      "metadata": {
        "id": "g2dfFBO08bMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging Data\n",
        "# Merge 'stores' with 'train' and 'test'\n",
        "train = pd.merge(train, stores, on='store_nbr', how='left')\n",
        "test = pd.merge(test, stores, on='store_nbr', how='left')\n",
        "\n",
        "# Merge 'oil' with 'train' and 'test'\n",
        "train = pd.merge(train, oil, on='date', how='left')\n",
        "test = pd.merge(test, oil, on='date', how='left')\n",
        "\n",
        "# Merge 'holidays' with 'train' and 'test'\n",
        "train = pd.merge(train, holidays, on='date', how='left')\n",
        "test = pd.merge(test, holidays, on='date', how='left')"
      ],
      "metadata": {
        "id": "zhayU6MF-vaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "1aChk8fOE8e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "zDMOlmtOE-cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.** **Feature Engineering**"
      ],
      "metadata": {
        "id": "HJd8FBoXQztV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Based Features"
      ],
      "metadata": {
        "id": "bRozxlPlSW7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here Extracting day, week, month, year, and day of the week\n",
        "train['day'] = train['date'].dt.day\n",
        "train['week'] = train['date'].dt.isocalendar().week\n",
        "train['month'] = train['date'].dt.month\n",
        "train['year'] = train['date'].dt.year\n",
        "train['dayofweek'] = train['date'].dt.dayofweek  # 0: Monday, 6: Sunday"
      ],
      "metadata": {
        "id": "CnbiK0eoJrHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying same to the test set\n",
        "test['day'] = test['date'].dt.day\n",
        "test['week'] = test['date'].dt.isocalendar().week\n",
        "test['month'] = test['date'].dt.month\n",
        "test['year'] = test['date'].dt.year\n",
        "test['dayofweek'] = test['date'].dt.dayofweek  # 0: Monday, 6: Sunday"
      ],
      "metadata": {
        "id": "S4gE-lJ6ROJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying Seasonal Trends\n",
        "# I analyzed sales data for all months to see if there's a trend\n",
        "december_sales = train[train['month'] == 12]['sales'].mean()\n",
        "print(f\"Average sales in December: {december_sales}\")\n",
        "\n",
        "november_sales = train[train['month'] == 11]['sales'].mean()\n",
        "print(f\"Average sales in November: {november_sales}\")\n",
        "\n",
        "october_sales = train[train['month'] == 10]['sales'].mean()\n",
        "print(f\"Average sales in October: {october_sales}\")\n",
        "\n",
        "september_sales = train[train['month'] == 9]['sales'].mean()\n",
        "print(f\"Average sales in September: {september_sales }\")\n",
        "\n",
        "august_sales = train[train['month'] == 8]['sales'].mean()\n",
        "print(f\"Average sales in August: {august_sales}\")\n",
        "\n",
        "july_sales = train[train['month'] == 7]['sales'].mean()\n",
        "print(f\"Average sales in July: {july_sales}\")\n",
        "\n",
        "june_sales = train[train['month'] == 6]['sales'].mean()\n",
        "print(f\"Average sales in June: {june_sales}\")\n",
        "\n",
        "may_sales = train[train['month'] == 5]['sales'].mean()\n",
        "print(f\"Average sales in May: {may_sales}\")\n",
        "\n",
        "april_sales = train[train['month'] == 4]['sales'].mean()\n",
        "print(f\"Average sales in April: {april_sales }\")\n",
        "\n",
        "march_sales = train[train['month'] == 3]['sales'].mean()\n",
        "print(f\"Average sales in March: {march_sales}\")\n",
        "\n",
        "february_sales = train[train['month'] == 2]['sales'].mean()\n",
        "print(f\"Average sales in February: {february_sales}\")\n",
        "\n",
        "january_sales = train[train['month'] == 1]['sales'].mean()\n",
        "print(f\"Average sales in January: {january_sales}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_OiO-65KRYBO",
        "outputId": "cdc675c8-6f4e-4dbf-c388-5de43aba7026"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a03a5d3eda44>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Identifying Seasonal Trends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# I analyzed sales data for all months to see if there's a trend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdecember_sales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average sales in December: {december_sales}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# By Analyzing the sales trends for all the months we got to know that\n",
        "# In December month the Average Sales is High compared to other months."
      ],
      "metadata": {
        "id": "uxSFYnZDUEIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Event-Based Features"
      ],
      "metadata": {
        "id": "HLbfRnoaWj7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary flags for holidays( for promotions, and economic events it is alreay in correct format)\n",
        "# Binary flag for holidays (using 'type_y' column)\n",
        "train['is_holiday'] = train['type_y'].notna().astype(int)\n",
        "\n",
        "# Government payday feature (using 'day' and 'date' columns)\n",
        "train['is_payday'] = ((train['day'] == 15) | (train['day'] == train['date'].dt.days_in_month)).astype(int)\n",
        "\n",
        "# Earthquake impact feature (using 'date' column)\n",
        "train['earthquake_impact'] = (train['date'] == pd.to_datetime('2016-04-16')).astype(int)"
      ],
      "metadata": {
        "id": "2eNuTnhgWaKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same code for testing set\n",
        "test['is_holiday'] = test['type_y'].notna().astype(int)\n",
        "\n",
        "test['is_payday'] = ((test['day'] == 15) | (test['day'] == test['date'].dt.days_in_month)).astype(int)\n",
        "\n",
        "test['earthquake_impact'] = (test['date'] == pd.to_datetime('2016-04-16')).astype(int)"
      ],
      "metadata": {
        "id": "-_F65U5XacET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rolling Statistics"
      ],
      "metadata": {
        "id": "tbq_2UVxlKPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving Averages and Rolling Standard Deviations\n",
        "train['sales_rolling_mean_7'] = train.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
        "train['sales_rolling_std_7'] = train.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.rolling(window=7, min_periods=1).std())\n",
        "# You can experiment with different window sizes(e.g., 14,30)"
      ],
      "metadata": {
        "id": "lE4jWYsNj9zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lagged Features\n",
        "train['sales_lag_7'] = train.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.shift(7))\n",
        "train['sales_lag_30'] = train.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.shift(30))\n",
        "# You can experiment with different lag periods (e.g., 14, 90) for lagged features"
      ],
      "metadata": {
        "id": "JgSfYOkdmzwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"We cannot compute moving averages, rolling standard deviations, or lagged features\n",
        "on the test set before model training. These features should be computed only from\n",
        "the training data and then applied to the test set properly to avoid data leakage.\"\"\""
      ],
      "metadata": {
        "id": "_wLeI5lxrr4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store-Specific Aggregations"
      ],
      "metadata": {
        "id": "x7lWIDMpsqgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Sales per Store Type\n",
        "avg_sales_by_type = train.groupby('type_x')['sales'].mean()  # Calculate average sales for each store type\n",
        "train = train.merge(avg_sales_by_type, on='type_x', how='left', suffixes=('', '_avg_by_type'))  # Merge back into train\n",
        "test = test.merge(avg_sales_by_type, on='type_x', how='left', suffixes=('', '_avg_by_type'))  # Merge into test\n",
        "#  When calculating the average sales per store type, you should use the same values\n",
        "# for both the training and test sets to avoid data leakage and ensure consistency."
      ],
      "metadata": {
        "id": "_oCj3tlksZ-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Selling Product Families per Cluster\n",
        "top_selling_by_cluster = train.groupby(['cluster', 'family'])['sales'].sum().reset_index()  # Total sales per family in each cluster\n",
        "top_selling_by_cluster = top_selling_by_cluster.sort_values(['cluster', 'sales'], ascending=[True, False])  # Sort by cluster and sales\n",
        "top_selling_by_cluster = top_selling_by_cluster.groupby('cluster').head(3)  # Get top 3 families per cluster"
      ],
      "metadata": {
        "id": "ryHKHIbXuG9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_selling_by_cluster.head(10)"
      ],
      "metadata": {
        "id": "2QajuVX0u0VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a feature indicating if a product family is top-selling in its cluster\n",
        "train = train.merge(top_selling_by_cluster[['cluster', 'family']], on=['cluster', 'family'], how='left', indicator=True)\n",
        "train['is_top_selling'] = (train['_merge'] == 'both').astype(int)  # 1 if top-selling, 0 otherwise\n",
        "train.drop('_merge', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "KixEfJsEu89t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.merge(top_selling_by_cluster[['cluster', 'family']], on=['cluster', 'family'], how='left', indicator=True)\n",
        "test['is_top_selling'] = (test['_merge'] == 'both').astype(int)  # 1 if top-selling, 0 otherwise\n",
        "test.drop('_merge', axis=1, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "-fIt5hvtvWqg",
        "outputId": "7a416599-173c-4fd2-8b8d-205667223780"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-337f382bc071>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_selling_by_cluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'family'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'family'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_top_selling'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_merge'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1 if top-selling, 0 otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_merge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "5jgDmy98vaOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "5WcOraCfven8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "JCE-89UPwVIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Unnecessary columns\n",
        "columns_to_remove = ['id','city', 'state', 'type_y', 'locale', 'locale_name', 'description', 'transferred']\n",
        "train = train.drop(columns=columns_to_remove)\n",
        "test = test.drop(columns=columns_to_remove)"
      ],
      "metadata": {
        "id": "H4Ex5QtT0MNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().sum() # To find total null values in training set"
      ],
      "metadata": {
        "id": "SF8OWowWyGe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values in 'dcoilwtico' using linear interpolation\n",
        "train['dcoilwtico'] = train['dcoilwtico'].interpolate(method='linear')\n",
        "test['dcoilwtico'] = test['dcoilwtico'].interpolate(method='linear')\n",
        "\n",
        "# Forward fill missing values in rolling statistics and lagged features\n",
        "for col in ['sales_rolling_std_7', 'sales_lag_7', 'sales_lag_30']:\n",
        "    train[col] = train[col].bfill()  # Backward fill"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "b8UuoQDfyKVs",
        "outputId": "b3c9e5d7-1145-480a-efb2-4f5b844cb20d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2c0a24660f49>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Impute missing values in 'dcoilwtico' using linear interpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dcoilwtico'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dcoilwtico'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dcoilwtico'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dcoilwtico'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Forward fill missing values in rolling statistics and lagged features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_colname=[]  # storing non numeric data types\n",
        "for x in train.columns:\n",
        "    if train[x].dtypes=='object':\n",
        "        train_colname.append(x)\n",
        "train_colname"
      ],
      "metadata": {
        "id": "2pE2YAThZL46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_colname=[]    # storing non numeric data types\n",
        "for x in test.columns:\n",
        "    if test[x].dtypes=='object':\n",
        "        test_colname.append(x)\n",
        "test_colname"
      ],
      "metadata": {
        "id": "ShtHMzQEaQq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le=LabelEncoder()\n",
        "\n",
        "for x in train_colname:\n",
        "    train[x]=le.fit_transform(train[x])\n",
        "for x in test_colname:\n",
        "    test[x]=le.fit_transform(test[x])"
      ],
      "metadata": {
        "id": "wKWw-DHxavmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Visualize Sales Trends Over Time:"
      ],
      "metadata": {
        "id": "Bpwlwy7_6pdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time series plot of overall sales\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='month', y='sales', data=train)\n",
        "plt.title('Overall Sales Trend Over Time')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dlsW9nFA2NGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sales trend by store type\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='month', y='sales', hue='type_x', data=train)\n",
        "plt.title('Sales Trend by Store Type')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kviw5HkP537k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sales trend for a specific product family\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='month', y='sales', data=train[train['family'] == 'GROCERY I'])\n",
        "plt.title('Sales Trend for GROCERY I')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PMvCdTzc6K0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Analyze Sales Before and After Holidays and Promotions:"
      ],
      "metadata": {
        "id": "f-0TGEaL6dc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sales before and after a specific holiday\n",
        "holiday_date = pd.to_datetime('2016-12-25')  # Example: Christmas\n",
        "sales_before_holiday = train[(train['date'] >= holiday_date - pd.DateOffset(days=7)) & (train['date'] < holiday_date)]['sales'].mean()\n",
        "sales_after_holiday = train[(train['date'] >= holiday_date) & (train['date'] < holiday_date + pd.DateOffset(days=7))]['sales'].mean()\n",
        "print(f\"Average sales before holiday: {sales_before_holiday}\")\n",
        "print(f\"Average sales after holiday: {sales_after_holiday}\")"
      ],
      "metadata": {
        "id": "DwNETsgD6VIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate average sales when products are on promotion\n",
        "avg_sales_on_promotion = train[train['onpromotion'] == 1]['sales'].mean()\n",
        "\n",
        "# Calculate average sales when products are not on promotion\n",
        "avg_sales_not_on_promotion = train[train['onpromotion'] == 0]['sales'].mean()\n",
        "\n",
        "# Print the comparison\n",
        "print(f\"Average sales on promotion: {avg_sales_on_promotion}\")\n",
        "print(f\"Average sales not on promotion: {avg_sales_not_on_promotion}\")"
      ],
      "metadata": {
        "id": "N5Pb4dNu7BgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Check Correlations Between Oil Prices and Sales Trends:"
      ],
      "metadata": {
        "id": "fhWUhzrpAMvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot of oil prices vs. sales\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='dcoilwtico', y='sales', data=train)\n",
        "plt.title('Oil Prices vs. Sales')\n",
        "plt.xlabel('Oil Price')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zT-UVSS5AP8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation coefficient\n",
        "correlation = train['dcoilwtico'].corr(train['sales'])\n",
        "print(f\"Correlation between oil prices and sales: {correlation}\") # Since dataset having more null values so correlation is less."
      ],
      "metadata": {
        "id": "P3bG118XAS86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Identify Anomalies in the Data:"
      ],
      "metadata": {
        "id": "CA_G57Q6AXEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plot to identify outliers in sales\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(y='sales', data=train)\n",
        "plt.title('Box Plot of Sales')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3dfoaGmJAaWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DAY 2**"
      ],
      "metadata": {
        "id": "0NlRGb6EHGi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Model Selection, Forecasting, and Evaluation"
      ],
      "metadata": {
        "id": "bwP6en3OHOaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Baseline Model (Nave Forecasting)\n",
        "train['naive_forecast'] = train['sales'].shift(1)\n",
        "baseline_mse = mean_squared_error(train['sales'][1:], train['naive_forecast'][1:])\n",
        "print(f\"Baseline MSE: {baseline_mse}\")\n",
        "predictions_naive = test['sales'].shift(1) # Assuming test set has previous sales for naive forecast"
      ],
      "metadata": {
        "id": "u7Duh0o1FXWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. ARIMA Model\n",
        "train_sample = train['sales'].iloc[-100000:]  # Use only the last 100000 data points\n",
        "model_arima = ARIMA(train_sample, order=(5, 1, 0))  # Adjust order as needed\n",
        "model_arima_fit = model_arima.fit()\n",
        "predictions_arima = model_arima_fit.predict(start=len(train), end=len(train) + len(test) - 1)"
      ],
      "metadata": {
        "id": "NqpEmmQEIrLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Random Forest Regressor\n",
        "X = train[['day', 'week', 'month', 'year', 'dayofweek', 'onpromotion', 'is_holiday', 'is_payday', 'earthquake_impact', 'dcoilwtico', 'sales_rolling_mean_7', 'sales_rolling_std_7', 'sales_lag_7', 'sales_lag_30', 'sales_avg_by_type', 'is_top_selling']]\n",
        "y = train['sales']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model_rf = RandomForestRegressor(n_estimators=30, max_depth=7, min_samples_split=5, min_samples_leaf=3,\n",
        "                                 n_jobs=-1, warm_start=True, random_state=42)   # Limit tree depth\n",
        "model_rf.fit(X_train, y_train)\n",
        "predictions_rf = model_rf.predict(X_val)"
      ],
      "metadata": {
        "id": "0JBjIoueREM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. XGBoost Model\n",
        "model_xgb = XGBRegressor(n_estimators=50,max_depth=4, n_jobs=-1, learning_rate=0.1, random_state=42)\n",
        "model_xgb.fit(X_train, y_train)\n",
        "predictions_xgb = model_xgb.predict(X_val)\n"
      ],
      "metadata": {
        "id": "8kxgtgkxWOWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. LSTM Model\n",
        "# 1. Select relevant features and target variable\n",
        "features = ['day', 'week', 'month', 'year', 'dayofweek', 'onpromotion', 'is_holiday', 'is_payday', 'earthquake_impact', 'dcoilwtico', 'sales_rolling_mean_7', 'sales_rolling_std_7', 'sales_lag_7', 'sales_lag_30', 'sales_avg_by_type', 'is_top_selling']\n",
        "target = 'sales'\n",
        "\n",
        "X = train[features].values\n",
        "y = train[target].values\n",
        "\n",
        "# Scale the data using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y.reshape(-1, 1))  # Reshape y for scaling\n",
        "\n",
        "# Reshape data for LSTM (samples, timesteps, features)\n",
        "# You need to determine the appropriate 'timesteps' value (e.g., number of previous days to consider)\n",
        "timesteps = 7  # Example: Using the past 7 days as input\n",
        "X_reshaped = []\n",
        "y_reshaped = []\n",
        "for i in range(timesteps, len(X)):\n",
        "    X_reshaped.append(X[i - timesteps:i])\n",
        "    y_reshaped.append(y[i])\n",
        "\n",
        "X_reshaped = np.array(X_reshaped)\n",
        "y_reshaped = np.array(y_reshaped)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create the LSTM Model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(LSTM(units=50))\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(Dense(units=1))  # Output layer for sales prediction\n",
        "\n",
        "# 3. Compile the Model\n",
        "model_lstm.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# 4. Fit the Model\n",
        "model_lstm.fit(X_train, y_train, epochs=10, batch_size=64)  # Adjust epochs and batch_size as needed\n",
        "\n",
        "# 5. Make Predictions\n",
        "predictions_lstm = model_lstm.predict(X_val)"
      ],
      "metadata": {
        "id": "5_TXxbBzZBH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Model Evaluation**"
      ],
      "metadata": {
        "id": "0GjGi8GelD6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'Baseline': predictions_naive,\n",
        "    'ARIMA': predictions_arima,\n",
        "    'Random Forest': predictions_rf,\n",
        "    'XGBoost': predictions_xgb,\n",
        "    'LSTM': predictions_lstm,\n",
        "}\n",
        "\n",
        "for model_name, predictions in models.items():\n",
        "    rmse = np.sqrt(mean_squared_error(test['sales'], predictions))\n",
        "    mape = mean_absolute_percentage_error(test['sales'], predictions)\n",
        "    r2 = r2_score(test['sales'], predictions)\n",
        "    print(f\"{model_name}: RMSE = {rmse:.2f}, MAPE = {mape:.2f}, R^2 = {r2:.2f}\")"
      ],
      "metadata": {
        "id": "iCN3wJsUlH9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual Inspection\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test['date'], test['sales'], label='Actual')\n",
        "for model_name, predictions in models.items():\n",
        "    plt.plot(test['date'], predictions, label=model_name)\n",
        "plt.title('Actual vs. Predicted Sales')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "m_n4r0UKlYS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. *Visualization*"
      ],
      "metadata": {
        "id": "DmiisX0dlvTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Plot Historical Sales and Predicted Sales:"
      ],
      "metadata": {
        "id": "KGRijyPUmqPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test['date'], test['sales'], label='Actual Sales')\n",
        "\n",
        "for model_name, predictions in models.items():\n",
        "    plt.plot(test['date'], predictions, label=model_name)\n",
        "\n",
        "plt.title('Historical vs. Predicted Sales')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jxklLZNBlyAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Compare Model Performances Using Error Metrics:"
      ],
      "metadata": {
        "id": "aoC_HIIGmwcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate RMSE, MAPE, and R-squared for each model\n",
        "rmse_values = []\n",
        "mape_values = []\n",
        "r2_values = []\n",
        "\n",
        "for model_name, predictions in models.items():\n",
        "    rmse = np.sqrt(mean_squared_error(test['sales'], predictions))\n",
        "    mape = mean_absolute_percentage_error(test['sales'], predictions)\n",
        "    r2 = r2_score(test['sales'], predictions)\n",
        "\n",
        "    rmse_values.append(rmse)\n",
        "    mape_values.append(mape)\n",
        "    r2_values.append(r2)\n",
        "\n",
        "# Create a bar plot to compare RMSE values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models.keys(), rmse_values)\n",
        "plt.title('Model Comparison - RMSE')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('RMSE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "po0EztXml4nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Visualize Feature Importance (for Random Forest/XGBoost):"
      ],
      "metadata": {
        "id": "ortLmUS6m5S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances from Random Forest\n",
        "feature_importances_rf = model_rf.feature_importances_\n",
        "\n",
        "# Get feature importances from XGBoost\n",
        "feature_importances_xgb = model_xgb.feature_importances_\n",
        "\n",
        "# Create a bar plot for Random Forest feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(X.columns, feature_importances_rf)\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8-3B4HcEm63m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Interpretation and Business Insights"
      ],
      "metadata": {
        "id": "CFn1_3_xnP0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the evaluation metrics (RMSE, MAPE, R-squared) and the computational constraints (lack of GPU), the XGBoost model demonstrated the best overall performance in the sales forecasting task, even though it was trained on a validation set rather than the final test set. While the provided code did not have code to apply XGBoost to the test set, it performed the best on the validation set, suggesting it may be the best overall.\n",
        "\n",
        "Reasons for XGBoost's Superior Performance:\n",
        "Lower RMSE and MAPE,\n",
        "Higher R-squared Score"
      ],
      "metadata": {
        "id": "ffM0Sg0KxIJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0dj7kuq_ytac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Influence of External Factors:**\n",
        "\n",
        "1.Promotions:\n",
        "Positive Impact: Promotions generally had a strong positive impact on sales predictions. This is evident from the feature importance analysis in tree-based models (Random Forest, XGBoost), where 'onpromotion' was often ranked as a highly influential feature.\n",
        "\n",
        "2.Holidays:\n",
        "Mixed Impact: Holidays had a mixed impact on sales predictions, depending on the specific holiday and the overall sales trends.\n",
        "\n",
        "Increased Sales: Some holidays, like Christmas and Easter, were associated with increased sales predictions, reflecting seasonal shopping patterns and consumer behavior.\n",
        "\n",
        "3.Oil Prices:\n",
        "Negative Correlation: Oil prices exhibited a slight negative correlation with sales predictions. This suggests that higher oil prices might lead to lower consumer spending, potentially due to increased transportation costs or reduced disposable income.\n",
        "\n",
        "Indirect Influence: The influence of oil prices might be indirect, affecting sales through broader economic factors or consumer sentiment.\n"
      ],
      "metadata": {
        "id": "_Y9L8o_3x4x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Strategies to Improve Sales Forecasting:**\n",
        "\n",
        "Seasonal Adjustments: Incorporate seasonality patterns into inventory planning, particularly around holidays and major events. Increase inventory for products expected to have higher demand during those periods.\n",
        "\n",
        "Promotion Optimization: Identify periods or product categories where promotions are likely to have the greatest impact on sales. Tailor promotional strategies based on predicted demand and customer segments.\n",
        "\n",
        "Dynamic Pricing: Consider implementing dynamic pricing strategies based on predicted demand and competitor analysis. Adjust prices in real-time to maximize revenue and profitability.\n",
        "\n",
        "Enhance Data Collection: Continuously improve data collection processes to capture more granular and relevant information about customer behavior, market trends, and external factors."
      ],
      "metadata": {
        "id": "-orHGuK2yuv2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-qGYY1yxHK8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}